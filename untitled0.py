# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tpxs_Q_hGfZd_5ZhiNscppOeVUMBPGSr
"""

# âœ… Part 1: Installations (Run Only Once in Colab)

!apt-get -y install poppler-utils tesseract-ocr
!pip install -q pytesseract pdf2image spacy gradio transformers nltk pandas
!python -m spacy download en_core_web_sm

import nltk
nltk.download('punkt')

# STEP 1: Imports
import os
import re
import spacy
import pytesseract
import gradio as gr
import pandas as pd
from pdf2image import convert_from_path
from transformers import pipeline
from PIL import Image
from io import BytesIO
from collections import defaultdict

#STEP 2: Load NLP + QA Models
# ----------------------------------------------
nlp = spacy.load("en_core_web_sm")
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

# STEP 3: OCR Text Extraction
def extract_text_from_pdf(pdf_path):
    text = ""
    images = convert_from_path(pdf_path, dpi=300)
    for img in images:
        img = img.convert("L")
        text += pytesseract.image_to_string(img, config="--psm 6")
    return text

# STEP 4: Contact and Name Extraction
def extract_phone_number(text):
    pattern = r'(?:\+91[\s\-]?)?[6-9]\d{4}[\s\-]?\d{5}'
    matches = re.findall(pattern, text)
    return list(set(match.strip() for match in matches))

def extract_email(text):
    return list(set(re.findall(r'[\w\.-]+@[\w\.-]+', text)))

TECH_WORDS = {
    "python", "java", "machine learning", "nlp", "react.js", "blockchain",
    "aws", "docker", "html", "css", "graphql", "typescript", "langchain",
    "firebase", "mongodb", "sql", "jenkins", "redux", "express.js", "postman"
}

def extract_name_header(text):
    # Only check first 10 lines for the candidate's name
    lines = text.strip().split("\n")[:10]
    for line in lines:
        clean = line.strip(" -â€“|â€¢.:Â·â€¢â–º").strip()
        if not clean:
            continue
        # Skip lines containing tech words or digits
        if any(word.lower() in TECH_WORDS for word in clean.split()):
            continue
        if any(char.isdigit() for char in clean):
            continue
        # Use NLP to detect PERSON entities
        doc = nlp(clean)
        if any(ent.label_ == "PERSON" for ent in doc.ents):
            return [clean]
    return ["Not detected"]

 #STEP 5: Address and Skill Extraction
def extract_address(text):
    lines = text.split('\n')
    keywords = ['road', 'nagar', 'lane', 'area', 'sector', 'city', 'pincode', 'state', 'district', 'india']
    address_lines = []
    for line in lines:
        if any(word in line.lower() for word in keywords) and len(line) < 80:
            address_lines.append(line.strip())
    return address_lines if address_lines else ['Not mentioned']

CUSTOM_SKILLS = set([
    'python', 'java', 'c++', 'sql', 'machine learning', 'deep learning', 'nlp',
    'data analysis', 'react.js', 'node.js', 'express.js', 'mongodb', 'firebase',
    'docker', 'aws', 'git', 'github', 'graphql', 'html', 'css', 'typescript', 'next.js',
    'redux', 'jenkins', 'postman', 'socket.io', 'langchain', 'openai', 'jwt authentication'
])

def extract_skills(text):
    text_lower = text.lower()
    return sorted([skill for skill in CUSTOM_SKILLS if skill in text_lower])

#STEP 6: ATS Resume Section Scoring
def score_resume_sections(text):
    sections = {
        "Education": ["education", "bachelor", "degree"],
        "Experience": ["experience", "intern", "company"],
        "Skills": ["skills", "languages", "technologies"],
        "Projects": ["project", "portfolio", "case study"]
    }
    scores = {}
    for section, keywords in sections.items():
        scores[section] = any(word in text.lower() for word in keywords)
    return {k: "âœ…" if v else "âŒ" for k, v in scores.items()}


#STEP 7: Gradio UI + QA Bot
resume_text = ""

def analyze_resume(file):
    global resume_text
    resume_text = extract_text_from_pdf(file.name)

    phones = extract_phone_number(resume_text)
    emails = extract_email(resume_text)
    names = extract_name_header(resume_text)
    address = extract_address(resume_text)
    skills = extract_skills(resume_text)
    score = score_resume_sections(resume_text)

    return f"""
ðŸ“± Phone: {phones}
ðŸ“§ Email: {emails}
ðŸ§‘ Name: {names}
ðŸ“ Address: {address}
ðŸ’¼ Skills: {skills}
ðŸ“Š ATS Score: {score}
"""

def answer_question(question):
    if not resume_text.strip():
        return "âŒ Upload and analyze a resume first."
    try:
        result = qa_model({"question": question, "context": resume_text})
        return result['answer']
    except:
        return "âŒ Could not find a confident answer."

with gr.Blocks() as demo:
    gr.Markdown("## ðŸ§  Resume Parser & QA Bot (Final Version)")

    with gr.Row():
        file_input = gr.File(file_types=['.pdf'], label="Upload Resume")
        analyze_btn = gr.Button("Analyze Resume")

    result_box = gr.Textbox(label="Extracted Resume Info", lines=10)

    with gr.Row():
        question_input = gr.Textbox(label="Ask a question")
        answer_output = gr.Textbox(label="Answer from Resume")

    analyze_btn.click(fn=analyze_resume, inputs=[file_input], outputs=[result_box])
    question_input.submit(fn=answer_question, inputs=[question_input], outputs=[answer_output])

# Launch the app
demo.launch()

